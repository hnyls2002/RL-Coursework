{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zV_LiEbybwjM",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 简介\n",
    "\n",
    "本次作业包含2个代码填空和2个Exercise。\n",
    "\n",
    "回顾课堂内容，我们知道TRPO在很多场景上都很成功，但是我们也发现了它的计算过程非常的复杂，每步更新的运算量非常大。于是，在2017年TRPO的改进版PPO算法被提出，它基于TRPO的思想，但是实现算法更加简单，避免了复杂的求KL散度的Hessian矩阵。并且大量的实验结果表明，PPO能够比TRPO学习的更快，这使得PPO一下子成为了非常流行的强化学习算法。如果我们想要尝试在一个新的环境用强化学习，那么PPO就属于那种可以首先尝试的算法。\n",
    "\n",
    "## PPO算法\n",
    "\n",
    "我们回忆一下TRPO的优化目标：\n",
    "$$\n",
    " \\max\\limits_{\\theta}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)\\right]\\quad\\text{s.t.}\\quad \\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\left[D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_\\theta(\\cdot|s))\\right]\\le\\delta\n",
    "$$\n",
    "\n",
    "TRPO使用泰勒展开近似、共轭梯度、线性搜索等方法直接求解。PPO的优化目标同样是它，但PPO用了一些相对简单的方法来求解。具体来说，PPO有两种形式，一是PPO-Penalty，二是PPO-Clip。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWW0yZsuc9kv",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### PPO-Penalty\n",
    "\n",
    "PPO-Penalty用Lagrange乘子法直接将KL散度的限制放进了目标函数中，这就变成了一个无约束的优化问题，然后在迭代的过程中不断更新KL散度前的系数。即\n",
    "$$\n",
    "\\theta=\\mathop{\\arg\\max}_\\theta \\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)-\\beta D_{KL}[\\pi_{\\theta_k}(\\cdot|s),\\pi_\\theta(\\cdot|s)]\\right]\n",
    "$$\n",
    "\n",
    "令 $d_k=D_{KL}^{\\nu^{\\pi_{\\theta_k}}}(\\pi_{\\theta_k},\\pi_\\theta)$，$\\beta$ 的更新规则如下：\n",
    "1. 如果$d_k<\\delta/1.5$，那么$\\beta_{k+1}\\leftarrow \\beta_k/2$\n",
    "2. 如果$d_k>\\delta\\times1.5$，那么$\\beta_{k+1}\\leftarrow \\beta_k\\times 2$\n",
    "3. 否则$\\beta_{k+1}=\\beta_k$。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5e0rjldxgkFA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### PPO-Clip\n",
    "\n",
    "PPO-Clip更加直接，其直接在目标函数里进行限制，以保证新的参数和旧的参数的差距不会太大，即\n",
    "\n",
    "$$\n",
    "\\theta=\\mathop{\\arg\\max}_\\theta \\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[\\min \\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a),\\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},1-\\epsilon,1+\\epsilon\\right)A^{\\pi_{\\theta_k}}(s,a)\\right)\\right]\n",
    "$$\n",
    "\n",
    "其中 $\\text{clip}(x,l,r):=\\max(\\min(x,r),l)$ ，即把 $x$ 限制在 $[l,r]$ 内。上式中$\\epsilon$是一个超参数，表示clip的范围。如果$A(s,a)>0$，说明这个动作的Q值高于平均，最大化这个式子会增大$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}$，但不会让其超过$1+\\epsilon$。反之，如果$A(s,a)<0$，最大化这个式子会减小$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}$，但不会让其超过$1-\\epsilon$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LsYckcOoDOUW",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# PPO代码实践\n",
    "\n",
    "我们在两个环境CartPle和Pendulum上测试PPO算法。实验表明，PPO-Clip总是比PPO-Penalty表现得更好。因此下面我们只出PPO-Clip的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EftJCiSI-DOk",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 环境1\n",
    "\n",
    "首先导入一些必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0-WdD7cQhd8",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们的PPOAgent主要有两个网络：policy网络和value网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGQDcsxLQ2D0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "  \n",
    "  \"\"\"创建两种agent，并设定部分超参数。\"\"\"\n",
    "  def __init__(self, feature_n, action_n):\n",
    "\n",
    "    self.policy_model = torch.nn.Sequential(torch.nn.Linear(feature_n, 30),\n",
    "                         torch.nn.Tanh(),\n",
    "                         torch.nn.Linear(30, action_n),\n",
    "                         torch.nn.Softmax(dim=1))\n",
    "    self.value_model = torch.nn.Sequential(torch.nn.Linear(feature_n, 30),\n",
    "                         torch.nn.ReLU(),\n",
    "                         torch.nn.Linear(30, 10),\n",
    "                         torch.nn.ReLU(),\n",
    "                         torch.nn.Linear(10, 1))\n",
    "\n",
    "    self.policy_optim = torch.optim.Adam(self.policy_model.parameters(), lr=1e-4)\n",
    "    self.value_optim = torch.optim.Adam(self.value_model.parameters(), lr=3e-4)\n",
    "\n",
    "    self.gamma = 0.98 # 折扣因子\n",
    "    self.batch_size = 128\n",
    "    self.eps = 0.2\n",
    "  \n",
    "  \"\"\"根据给定的状态，采样动作。\"\"\"\n",
    "  def sample_action(self, state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "    prob = self.policy_model(state_tensor)\n",
    "    dist = torch.distributions.Categorical(probs=prob)\n",
    "    return dist.sample(), prob\n",
    "  \n",
    "  \"\"\"根据给定的状态，计算V函数。\"\"\"\n",
    "  def get_value(self, state):\n",
    "    return self.value_model(state)\n",
    "\n",
    "  \"\"\"策略学习。\"\"\"\n",
    "  def policy_learn(self, s, a, old_pro, adv):\n",
    "    ########################################\n",
    "    ## Programming 1:策略更新\n",
    "    ########################################\n",
    "    prob_ratio = torch.zeros(1)\n",
    "    ########################################\n",
    "    ## End of Programming 1\n",
    "    ########################################\n",
    "    return prob_ratio  # 返回probability ratio以观察训练过程\n",
    "\n",
    "  \"\"\"价值学习。\"\"\"\n",
    "  def value_learn(self, s, r, d, s_):\n",
    "    v_ = self.get_value(s_)\n",
    "    v = self.get_value(s)\n",
    "    td_error = v - (r + (1-d) * self.gamma * v_).detach()\n",
    "    loss = td_error.pow(2).mean()\n",
    "\n",
    "    self.value_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(self.value_model.parameters(), .5)\n",
    "    self.value_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvYSOuYVCMW8",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "计算advantage函数的时候，对于Q函数的计算做一步展开（TD）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Z9eSRUa6r-y",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_advantage(s, s_, r, d, agent):\n",
    "  with torch.no_grad():\n",
    "    q_fn = r + (1-d) * agent.gamma * agent.value_model(s_)\n",
    "  return q_fn - agent.value_model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7lT6DolRpvg",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "创建环境，并设定随机数种子以便重复实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "seed = 999\n",
    "env.reset(seed=seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2DQG4Z6Rpvm",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "实例化agent并开始训练。在训练过程中，我们会动态描绘训练曲线（横坐标是episode，纵坐标是对应的reward）。\n",
    "\n",
    "当训练出的agent已经足够好时，训练停止并输出“Solved!”。\n",
    "\n",
    "期望运行时间：$1$分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = 500\n",
    "agent = PPOAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "epi_n = 1500\n",
    "mini_epoch = 15\n",
    "i_list = []\n",
    "r_list = []\n",
    "p_list = []\n",
    "p_i_list = []\n",
    "\n",
    "s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epi_n + 1):\n",
    "  state = env.reset()[0]\n",
    "  done = False\n",
    "  \n",
    "  tot_reward = 0\n",
    "  while not done:\n",
    "    action, action_distribution = agent.sample_action([state])\n",
    "    next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "    done = terminated or truncated\n",
    "    tot_reward += reward\n",
    "\n",
    "    if done: \n",
    "      reward = -20\n",
    "\n",
    "    s.append(state) \n",
    "    a.append(action) \n",
    "    p.append(action_distribution) \n",
    "    r.append(reward) \n",
    "    s_.append(next_state) \n",
    "    d.append(done)\n",
    "\n",
    "    state = next_state\n",
    "  \n",
    "  if len(s) >= 200:\n",
    "    # print(f\"collect {len(s)} samples\")\n",
    "    s = torch.tensor(s, dtype = torch.float)\n",
    "    a = torch.tensor(a, dtype = torch.long).view(-1, 1)\n",
    "    p = torch.cat(p).view(-1, 2)\n",
    "    r = torch.tensor(r, dtype = torch.float).view(-1, 1)\n",
    "    s_ = torch.tensor(s_, dtype = torch.float)\n",
    "    d = torch.tensor(d, dtype = torch.float).view(-1, 1)\n",
    "\n",
    "    adv = compute_advantage(s, s_, r, d, agent).detach().view(-1, 1)\n",
    "\n",
    "    prob_ratio = 0\n",
    "    for _ in range(mini_epoch):\n",
    "      prob_list = []\n",
    "      for idx in torch.utils.data.sampler.BatchSampler(\n",
    "          torch.utils.data.sampler.SubsetRandomSampler(range(len(s))), agent.batch_size, False):\n",
    "        prob = agent.policy_learn(s[idx], a[idx], p[idx], adv[idx])\n",
    "        agent.value_learn(s[idx], r[idx], d[idx], s_[idx])\n",
    "        prob_list.append(prob.mean().item())\n",
    "      prob_ratio += np.mean(prob_list)\n",
    "    p_list.append(prob_ratio / mini_epoch)\n",
    "    p_i_list.append(i)\n",
    "    # on-policy 训练\n",
    "    s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "  # 画图\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    plt.figure()\n",
    "    plt.plot([0, i], [line, line])\n",
    "    i_list.append(i)\n",
    "    tot_reward_list = []\n",
    "    for _ in range(5):\n",
    "      tot_reward = 0\n",
    "      state = env.reset()[0]\n",
    "      done = False\n",
    "      while not done:\n",
    "        _, prob = agent.sample_action([state])\n",
    "        action = prob.argmax(-1)\n",
    "        state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        tot_reward += reward\n",
    "      tot_reward_list.append(tot_reward)\n",
    "    r_list.append(np.mean(tot_reward_list))\n",
    "    plt.plot(i_list, r_list)\n",
    "    clear_output(True)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('accumulated reward')\n",
    "    plt.figure()\n",
    "    plt.plot(p_i_list, p_list)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('prob ratio')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A30741B84E57449897C117A343762ADE",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "609a3a8206b94200178e6452",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "参考训练过程如下：\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/rt/w39tuKE-Rwv_/qsxj2k8ece.png)\n",
    "\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/rt/w39tuKE-Rwv_/qsxj2kvekg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1472BFC9EF764DDD83B2CE0F05D24D0F",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "60980a9506b94200178d55c4",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "在上面的图中，probability ratio的值可以用来作为mini-epoch，learning rate这些参数调节的依据。离散动作空间中，在训练中期，该值可以说明策略更新的幅度，且应该保持在较大的值。而在下面的连续动作空间中，由于假设习得的动作分布是高斯分布，该值一般较为稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WI-XslbS3vep",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 连续动作环境\n",
    "\n",
    "对于连续动作环境，我们需要对上面的代码做一定的修改。具体来说，需要修改以下代码块（直接运行即可）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmpa0Z2u-GbM",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "下面是新的policy网络，因为环境是连续动作的，因此我们的网络分别输出表示动作分布的高斯分布的 $\\mu$ 和 $\\sigma$ 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DE-fFxkrCZMj",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolicyModel(torch.nn.Module):\n",
    "  def __init__(self, feature_n):\n",
    "    super(PolicyModel, self).__init__()\n",
    "    self.l1 = torch.nn.Linear(feature_n, 100)\n",
    "    self.l2 = torch.nn.Linear(100, 1)\n",
    "    self.l3 = torch.nn.Linear(100, 1)\n",
    "  def forward(self, x):\n",
    "    x = torch.nn.functional.relu(self.l1(x))\n",
    "    mu = 2 * torch.tanh(self.l2(x))\n",
    "    sigma = torch.nn.functional.softplus(self.l3(x))\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4964BJCdQmZD",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "以及修改一些主函数。\n",
    "\n",
    "这和上面的PPOAgent是类似的，只有部分修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfX-xwZjAO9i",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "  \n",
    "  def __init__(self, feature_n, action_n):\n",
    "\n",
    "    self.policy_model = PolicyModel(feature_n)\n",
    "    self.value_model = torch.nn.Sequential(torch.nn.Linear(feature_n, 100),\n",
    "                         torch.nn.ReLU(),\n",
    "                         torch.nn.Linear(100, 1))\n",
    "\n",
    "    self.policy_optim = torch.optim.Adam(self.policy_model.parameters(), lr=1e-4)\n",
    "    self.value_optim = torch.optim.Adam(self.value_model.parameters(), lr=3e-4)\n",
    "\n",
    "    self.gamma = 0.9 # 折扣因子\n",
    "    self.batch_size = 128\n",
    "    self.eps = 0.2\n",
    "  \n",
    "  def sample_action(self, state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "      mu, sigma = self.policy_model(state_tensor)\n",
    "    dist = torch.distributions.normal.Normal(mu, sigma)\n",
    "    action = dist.sample()\n",
    "    action_log_prob = dist.log_prob(action)\n",
    "    return action.item(), action_log_prob.item(), dist\n",
    "  \n",
    "  def get_value(self, state):\n",
    "    return self.value_model(state)\n",
    "\n",
    "  def policy_learn(self, s, a, old_p, adv):\n",
    "    ########################################\n",
    "    ## Programming 2:策略更新\n",
    "    ########################################\n",
    "    prob_ratio = torch.zeros(1)\n",
    "    ########################################\n",
    "    ## End of Programming 2\n",
    "    ########################################\n",
    "    return prob_ratio\n",
    "\n",
    "  def value_learn(self, s, r, d, s_):\n",
    "    v_ = self.get_value(s_)\n",
    "    v = self.get_value(s)\n",
    "    td_error = v - (r + (1-d) * self.gamma * v_).detach()\n",
    "    loss = td_error.pow(2).mean()\n",
    "\n",
    "    self.value_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(self.value_model.parameters(), .5)\n",
    "    self.value_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-X3mRwuCaHG",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "创建环境Pendulum-v1，并设定随机数种子以便重复实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Pendulum-v1\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "seed = 0\n",
    "env.reset(seed=seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwSQw7s8_Vry",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "实例化agent并开始训练。在训练过程中，我们会动态描绘训练曲线（横坐标是episode，纵坐标是对应的reward）。\n",
    "\n",
    "当训练出的agent已经足够好时，训练停止并输出“Solved!”。\n",
    "\n",
    "期望运行时间：$3$分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = -200\n",
    "\n",
    "agent = PPOAgent(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "epi_n = 1500\n",
    "mini_epoch = 15\n",
    "i_list = []\n",
    "r_list = []\n",
    "p_list = []\n",
    "p_i_list = []\n",
    "s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epi_n + 1):\n",
    "  state = env.reset()[0]\n",
    "  done = False\n",
    "  \n",
    "  tot_reward = 0\n",
    "  while not done:\n",
    "    action, action_distribution,_ = agent.sample_action([state])\n",
    "    next_state, reward, terminated, truncated, info = env.step([action])\n",
    "    done = terminated or truncated\n",
    "    tot_reward += reward\n",
    "\n",
    "    s.append(state) \n",
    "    a.append(action) \n",
    "    p.append(action_distribution) \n",
    "    r.append(reward) \n",
    "    s_.append(next_state) \n",
    "    d.append(done)\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "  if len(s) >= 1000:\n",
    "    s = torch.tensor(s, dtype = torch.float)\n",
    "    a = torch.tensor(a, dtype = torch.float).view(-1, 1)\n",
    "    p = torch.tensor(p, dtype = torch.float).view(-1, 1)\n",
    "    r = torch.tensor(r, dtype = torch.float).view(-1, 1)\n",
    "    s_ = torch.tensor(s_, dtype = torch.float)\n",
    "    d = torch.tensor(d, dtype = torch.float).view(-1, 1)\n",
    "\n",
    "    r = (r - r.mean()) / (r.std() + 1e-8)\n",
    "\n",
    "    adv = compute_advantage(s, s_, r, d, agent).detach().view(-1, 1)\n",
    "\n",
    "    prob_ratio = 0\n",
    "    for _ in range(mini_epoch):\n",
    "      prob_list = []\n",
    "      for idx in torch.utils.data.sampler.BatchSampler(\n",
    "          torch.utils.data.sampler.SubsetRandomSampler(range(len(s))), agent.batch_size, False):\n",
    "        prob = agent.policy_learn(s[idx], a[idx], p[idx], adv[idx])\n",
    "        agent.value_learn(s[idx], r[idx], d[idx], s_[idx])\n",
    "        prob_list.append(prob.mean().item())\n",
    "      prob_ratio += np.mean(prob_list)\n",
    "    p_list.append(prob_ratio / mini_epoch)\n",
    "    p_i_list.append(i)\n",
    "    # on-policy 训练\n",
    "    s, a, p, r, s_, d = [], [], [], [], [], []\n",
    "\n",
    "  # 画图\n",
    "  if i % 100 == 0:\n",
    "    plt.figure()\n",
    "    plt.plot([0, i], [line, line])\n",
    "    i_list.append(i)\n",
    "    tot_reward_list = []\n",
    "    for _ in range(5):\n",
    "      tot_reward = 0\n",
    "      state = env.reset()[0]\n",
    "      done = False\n",
    "      while not done:\n",
    "        _, _, prob = agent.sample_action([state])\n",
    "        action = prob.mean\n",
    "        state, reward, terminated, truncated, _ = env.step([action.item()])\n",
    "        done = terminated or truncated\n",
    "        tot_reward += reward\n",
    "      tot_reward_list.append(tot_reward)\n",
    "    r_list.append(np.mean(tot_reward_list))\n",
    "    plt.plot(i_list, r_list)\n",
    "    clear_output(True)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('accumulated reward')\n",
    "    plt.figure()\n",
    "    plt.plot(p_i_list, p_list)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('prob ratio')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBA79A918BA44B07868766BD3505F634",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "609a3a8206b94200178e6452",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "参考训练过程如下：\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/rt/qZvBJCNXGtTo/qsxjcb2dda.png)\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/rt/qZvBJCNXGtTo/qsxjcb6mg2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E3B941164DD4693AF1D314270F5F1E0",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "60980a9506b94200178d55c4",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 更新目标理论\n",
    "学习完这两章，现在我们给出一些关于策略梯度算法的更新目标的理论：\n",
    "首先，由策略梯度定理（证明见策略梯度章节的作业），我们知道：\n",
    "$$\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)] = \n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right)\\left(\\sum_{t^{\\prime}=t}^{T-1} r_{t^{\\prime}}\\right)\\right] \n",
    "\\tag{1}\n",
    "$$\n",
    "对比PPO的更新目标（这里使用GAE($\\lambda,0)$）$\\mathbb{E}_{s_t,s_{t+1} \\sim \\pi_{\\theta}}[r_t+\\gamma V(s_{t+1})-V(s_{t})]$和公式(1)，发现其中的差异主要在于减去了一项$V(s_{t})$，这一项通常被称为Baseline，此项一般只与状态有关。\n",
    "\n",
    "将Baseline表示为$b(s_t)$，引入Baseline，有\n",
    "$$\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)] = \n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right)\\left(\\sum_{t^{\\prime}=t}^{T-1} r_{t^{\\prime}}-b(s_t)\\right)\\right] \n",
    "\\tag{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3AC3CD6C4934D4B8201B37BCF2DD41F",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "60980a9506b94200178d55c4",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Exercise 1**. 证明\n",
    "$$\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) b\\left(s_{t}\\right)\\right] = 0 \\tag{3}.\n",
    "$$\n",
    "\n",
    "提示1：$\\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) b\\left(s_{t}\\right)\\right]=\\mathbb{E}_{s_{0: t}, a_{0: t-1}}\\left[\\mathbb{E}_{s_{t+1: T}, a_{t: T-1}}\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) b\\left(s_{t}\\right)\\right]\\right]$.\n",
    "\n",
    "提示2：要**证明** $\\mathbb{E}_{a_{t}}\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right)\\right]=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**.\n",
    "证明\n",
    "1. 求出最小化$Var(\\widehat{\\nabla_{\\theta} J})$的$b^{*}(s^{k}_t)$, 并指出其在何种条件下等于$\\mathbb{E}_{\\tau}\\left[\\sum_{t^{\\prime}=t}^{T-1} r^{k}_{t^{\\prime}}(s_{t^{\\prime}}^k, a_{t^{\\prime}}^k)\\right]=V(s^k_t)$.\n",
    "2. 记不用Baseline的梯度估计为\n",
    "$$\n",
    "\\widehat{\\nabla_{\\theta} J}^{\\prime} = \\frac{1}{N}\\sum_{k=0}^{N} \\left[\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t}^{k} \\mid s_{t}^{k}\\right)\\left(\\sum_{t^{\\prime}=t}^{T-1} r^{k}_{t^{\\prime}}\\right)\\right]. \\tag{5}\n",
    "$$\n",
    "证明使用$b^{*}$的梯度估计方差小于该不用baseline的梯度估计方差。\n",
    "\n",
    "提示1：$Var(X)=\\mathbb{E}(X^2)-(\\mathbb{E}(X))^2$, $Var(X+Y) = Var(X)+Var(Y)+2Cov(X,Y)$\n",
    "\n",
    "提示2：使用**Ex 1**的结论。\n",
    "\n",
    "提示3： 最小二乘法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
