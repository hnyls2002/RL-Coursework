{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C0480D97F344544BC2824225ADBF12D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 策略梯度算法\n",
    "\n",
    "本次作业包含2个代码填空和3个Exercise。\n",
    "\n",
    "# 简介\n",
    "之前我们介绍的Q-learning和DQN及改进算法都是基于价值（value-based）的方法，其中Q-learning是处理有限状态的算法，而DQN可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，此时并不存在一个显式的策略；而基于策略的方法则是直接显式的学习一个目标策略。策略梯度是基于策略的方法的基础，我们将从策略梯度算法说起。\n",
    "# 策略梯度\n",
    "基于策略的方法首先需要参数化策略，我们假设目标策略$\\pi_\\theta$是一个随机性策略，并且处处可微，其中$\\theta$是对应的参数。我们的目标是要寻找一个最优策略，来最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为\n",
    "$$\n",
    "J(\\theta)= \\mathbb{E}_{s_0}\\left[V^{\\pi_\\theta}(s_0)\\right]\n",
    "$$\n",
    "其中$s_0$表示初始状态。现在有了目标函数，我们将目标函数对策略$\\theta$求导，得到导数后，我们就可以用梯度上升方法来最大化这个目标函数从而得到最优策略。\n",
    "\n",
    "我们之前在MDP章节中学习过在策略$\\pi$下的状态访问分布，我们在此用$\\nu^{\\pi}$表示。然后我们对目标函数求梯度，可以得到如下式子，更详细的推导将在扩展阅读中给出。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta}J(\\theta)\n",
    "&\\propto \\sum_{s \\in S}\\nu^{\\pi_{\\theta}}(s)\\sum_{a \\in A}Q^{\\pi_{\\theta}}(s,a)\\nabla_{\\theta}\\pi_{\\theta}(a|s)\\\\\n",
    "&=\\sum_{s \\in S}\\nu^{\\pi_{\\theta}}(s)\\sum_{a \\in A}\\pi_{\\theta}(a|s)Q^{\\pi_{\\theta}}(s,a)\\frac{\\nabla_{\\theta}\\pi_{\\theta}(a|s)}{\\pi_{\\theta}(a|s)}\\\\\n",
    "&= \\mathbb{E}_{\\pi_{\\theta}}[Q^{\\pi_{\\theta}}(s,a)\\nabla_{\\theta}\\log \\pi_{\\theta}(a|s)]\n",
    "\\end{align}\n",
    "$$\n",
    "于是，我们就可以用这个梯度来更新策略。需要注意的是，因为上式期望$\\mathbb{E}$的下标是$\\pi_{\\theta}$，所以策略梯度算法为在线策略（on-policy）算法，即必须使用当前策略$\\pi_\\theta$采样得到的数据来计算梯度。更一般地，我们可以把梯度写成下面这个形式：\n",
    "$$\n",
    "g = \\mathbb{E}_{\\pi_{\\theta}}[\\sum^{\\infty}_{t=0}\\psi_{t}\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})]\n",
    "$$\n",
    "其中$\\psi_{t}$可以有很多种形式：\n",
    "$$\n",
    "\\begin{align}\n",
    "&1.\\psi_{t}=\\sum_{t=0}^{\\infty}\\gamma^t r_{t} : 轨迹的总回报  &&4.Q^{\\pi_{\\theta}}(s_{t},a_{t}) : 动作价值函数\\\\\n",
    "&2.\\psi_{t}=\\sum_{t'=t}^{\\infty} \\gamma^{t'-t} r_{t'} : 动作 a_{t}之后的回报 &&5.A^{\\pi_{\\theta}}(s_{t},a_{t}): 优势函数\\\\\n",
    "&3.\\psi_{t}=\\sum_{t'=t}^{\\infty}r_{t'}-b(s_{t}) : 基准线版本的改进 \\quad\\quad \\quad\\quad &&6.r_{t} + V^{\\pi_{\\theta}}(s_{t+1}) - V^{\\pi_{\\theta}}(s_t) : 时序差分残差\n",
    "\\end{align}\n",
    "$$\n",
    "在计算策略梯度的公式中，我们需要用到$Q^{\\pi_{\\theta}}(s,a)$，可以用多种方式对它进行估计。接下来要介绍的 REINFORCE 算法便是采用了蒙特卡洛方法来估计$Q^{\\pi_{\\theta}}(s,a)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** 请简单阐述策略梯度方法与基于值函数的方法有何区别。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE 算法\n",
    "REINFORCE就是在上文中当$\\psi_t=\\sum_{t'=t}^{\\infty}\\gamma^{t'-t}r_{t'}$时的策略梯度算法。\n",
    "具体流程如下：\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\cdot 初始化策略参数\\theta \\\\\n",
    "&\\cdot \\texttt{for}\\quad 序列\\quad e=1\\to E\\quad \\texttt{do}: \\\\\n",
    "&\\cdot\\qquad 用当前策略\\pi_\\theta采样轨迹\\{s_{1},a_{1},r_{1},s_{2},a_{2},r_{2} ... s_{t},a_{t},r_{t}\\}\\\\\n",
    "&\\cdot\\qquad 计算当前轨迹每个时刻t往后的回报\\sum_{t'=t}^{\\infty}\\gamma^{t'-t}r_{t'}，记为\\psi_{t}\\\\\n",
    "&\\cdot\\qquad 对\\theta进行更新 \\theta = \\theta + \\alpha \\sum_{t=0}^{\\infty}\\psi_{t}\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})\\\\\n",
    "&\\cdot\\texttt{end for}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2F59DF8812534F36874771F8720C93B7",
    "jupyter": {},
    "notebookId": "5f8f327c46ba5e00307827a4",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -i https://mirrors.sjtug.sjtu.edu.cn/pypi/web/simple gym # 注意gym版本为0.26.2\n",
    "%pip install torch # No need to install cuda toolkit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "40B616B26E8B4AA48E98B4258C7988CA",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C0E3A3A844549BC905ED109D604EF92",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "定义我们的策略网络PolicyNet，输入是状态state，输出则是采取动作action的概率值（离散动作空间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "864658B0470F4AB4823322580ED41CDA",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, state_space, action_space):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        input_dim = state_space.shape[0]\n",
    "        output_dim = action_space.n\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return  F.softmax(self.fc2(x), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A19E6CFCEDC467BA350D1200C7E28B6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "再定义我们的REINFORCE的Agent。在函数action()中，我们通过概率对离散的动作进行采样。在更新Agent的过程中，我们按照算法，将损失函数写为$-\\sum_{t}\\psi_{t}\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})$，对$\\theta$求导就可以更新策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "677551348C8D4AAE8AE44C4B46AEFA26",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reinforce:\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_size, state_space, action_space, \n",
    "        learning_rate, device, step_size, lr_gamma\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.model = PolicyNet(hidden_size, state_space, action_space).to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        # learning rate调节\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size, lr_gamma)\n",
    "\n",
    "    def action(self, state):\n",
    "        ########################################\n",
    "        ## Programming 1: 计算正确的action以及log_prob\n",
    "        ########################################\n",
    "        action = torch.tensor([0]).to(self.device)\n",
    "        log_prob = torch.tensor([0.0], requires_grad=True).to(self.device)\n",
    "        ########################################\n",
    "        ## End of Programming 1\n",
    "        ########################################\n",
    "        return action, log_prob\n",
    "\n",
    "    def update(self, rewards, log_probs, gamma):\n",
    "        ########################################\n",
    "        ## Programming 2: 更新策略网络\n",
    "        ## REINFORCE算法通常不太稳定，如果得到的结果不理想可以尝试：\n",
    "        ## 1. 对**Return**做标准化\n",
    "        ## 2. 使用learning rate调节器\n",
    "        ########################################\n",
    "        pass\n",
    "        ########################################\n",
    "        ## End of Programming 2\n",
    "        ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6DDA156DE31493CA22217FA65E3C43E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "定义好策略，我们就可以开始实验了，看看Reinforce在Cartpole环境上表现如何吧！\n",
    "\n",
    "预计运行时间：2分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9993AF344AE4770896E0504E77BAF08",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 以下参数在助教实现的版本中能达到期望效果，可以多试几组参数\n",
    "learning_rate = 1e-3\n",
    "num_episode = 2000\n",
    "hidden_size = 128\n",
    "max_timesteps = 2000\n",
    "env_name = \"CartPole-v1\"\n",
    "gamma = 0.98\n",
    "lr_gamma = 0.9\n",
    "step_size = 500\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "env = gym.make(env_name)\n",
    "agent = Reinforce(\n",
    "    hidden_size, \n",
    "    env.observation_space, \n",
    "    env.action_space, \n",
    "    learning_rate, \n",
    "    device,\n",
    "    step_size,\n",
    "    lr_gamma\n",
    ")\n",
    "rewards_log = []\n",
    "episodes_log = []\n",
    "for i_episode in range(num_episode):\n",
    "    state = torch.Tensor(np.array([env.reset()[0]]), device=device)\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    for time_step in range(max_timesteps):\n",
    "        action, log_prob = agent.action(state)\n",
    "        action = action.cpu()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.numpy()[0])\n",
    "        done = terminated or truncated\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        state = torch.Tensor([next_state], device=device)\n",
    "        if done:\n",
    "            break\n",
    "    rewards_log.append(np.sum(rewards))\n",
    "    episodes_log.append(i_episode)\n",
    "    agent.update(rewards, log_probs, gamma)\n",
    "    if (i_episode + 1) % 100 == 0 or i_episode + 1 == num_episode:\n",
    "        print(\"Episode: {}, Reward: {}\".format(i_episode+1, np.mean(rewards_log[-10:])))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB975960A51D4D97A708F800DAEB03EE",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "在CartPole-v1环境中，满分就是500分，让我们来看看每个Episode得分如何吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2B1AD392B66419791F4FD6CB800A4A9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(episodes_log,rewards_log)\n",
    "plt.xlabel('Episodes')   \n",
    "plt.ylabel('Rewards per episode')  \n",
    "plt.title('Reinforce on {}'.format(env_name))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8CF05803C4D491FB9A8684DEC5BF217",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "6073bce2d143c8001737e6a4",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "参考训练曲线如下（最后能比较稳定保持在500即可）：\n",
    "\n",
    "![Image Name](images/result.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** 在填写代码并训练的过程，你认为REINFORCE算法存在哪些问题？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7BF78B87D294A038E9BEA15C75E8940",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 总结\n",
    "REINFORCE算法理论上是能保证局部最优的。依赖于MC方法进行采样，优点是REINFORCE的采样梯度是无偏的。但是同样由于MC，导致REINFORCE梯度估计的方差很大，从而可能会降低学习的速率，这也是接下来的Actor-Critic算法要解决的问题。\n",
    "\n",
    "# 拓展阅读：策略梯度证明\n",
    "我们要证明$\\nabla_{\\theta}J(\\theta) \\propto \\sum_{s \\in S}\\nu^{\\pi_\\theta}(s)\\sum_{a \\in A}Q^{\\pi_\\theta}(s,a)\\nabla_{\\theta}\\pi_{\\theta}(a|s)$\n",
    "                        \n",
    "先从状态价值函数的推导开始：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta}V^{\\pi_\\theta}(s) &=\\nabla_{\\theta}(\\sum_{a \\in A} \\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a)) \\\\\n",
    "&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a) + \\pi_{\\theta}(a|s)\\nabla_{\\theta}Q^{\\pi_\\theta}(s,a))\\\\\n",
    "&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a) + \\pi_{\\theta}(a|s)\\nabla_{\\theta}\\sum_{s',r}P(s',r|s,a)(r+\\gamma V^{\\pi_\\theta}(s'))\\\\\n",
    "&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a) + \\gamma\\pi_{\\theta}(a|s)\\sum_{s',r}P(s',r|s,a)\\nabla_{\\theta}V^{\\pi_\\theta}(s'))\\\\\n",
    "&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a) + \\gamma\\pi_{\\theta}(a|s)\\sum_{s'}P(s'|s,a)\\nabla_{\\theta}V^{\\pi_\\theta}(s'))\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "为了简化表示，我们让$\\phi(s)=\\sum_{a \\in A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi_\\theta}(s,a)$, 定义$\\rho^{\\pi}(s\\rightarrow x, k)$为策略$\\pi$从状态s出发k步后到达状态x的概率，我们继续推导:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta}V^{\\pi_\\theta}(s) &= \\phi(s) + \\gamma\\sum_{a}\\pi_{\\theta}(a|s)\\sum_{s'}P(s'|s,a)\\nabla_{\\theta}V^{\\pi_\\theta}(s')\\\\\n",
    "&= \\phi(s) + \\gamma\\sum_{a}\\sum_{s'}\\pi_{\\theta}(a|s)P(s'|s,a)\\nabla_{\\theta}V^{\\pi_\\theta}(s')\\\\\n",
    "&= \\phi(s) + \\gamma\\sum_{s'}\\rho^{\\pi_\\theta}(s \\rightarrow s',1)\\nabla_{\\theta}V^{\\pi_\\theta}(s')\\\\\n",
    "&= \\phi(s) + \\gamma\\sum_{s'}\\rho^{\\pi_\\theta}(s \\rightarrow s',1)[\\phi(s') + \\sum_{s''}\\rho^{\\pi_\\theta}(s' \\rightarrow s'',1)\\nabla_{\\theta}V^{\\pi_\\theta}(s'')]\\\\\n",
    "&= \\phi(s) + \\gamma\\sum_{s'}\\rho^{\\pi_\\theta}(s \\rightarrow s',1)\\phi(s') + \\sum_{s''}\\rho^{\\pi_\\theta}(s \\rightarrow s'',2)\\nabla_{\\theta}V^{\\pi_\\theta}(s'')\\\\\n",
    "&= \\phi(s) + \\gamma\\sum_{s'}\\rho^{\\pi_\\theta}(s \\rightarrow s',1)\\phi(s') +\\gamma^2\\sum_{s''}\\rho^{\\pi_\\theta}(s' \\rightarrow s'',2)\\phi(s'') + \\gamma^3\\sum_{s'''}\\rho^{\\pi_\\theta}(s \\rightarrow s''',3)\\nabla_{\\theta}V^{\\pi_\\theta}(s''')\\\\\n",
    "&= ......\\\\\n",
    "&= \\sum_{x \\in S}\\sum^{\\infty}_{k=0}\\gamma^k\\rho^{\\pi_\\theta}(s \\rightarrow x, k)\\phi(x)\n",
    "\\end{align}\n",
    "$$\n",
    "OK! 我们定义$\\eta(s)= \\mathbb{E}_{s_0}\\left[\\sum^{\\infty}_{k=0}\\gamma^k\\rho^{\\pi}(s_{0} \\rightarrow s, k)\\right]$。\n",
    "至此我们看回我们的目标函数：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta}J(\\theta) &= \\nabla_{\\theta}\\mathbb{E}_{s_0}\\left[V^{\\pi_\\theta}(s_{0})\\right]\\\\\n",
    "&= \\sum_{s}\\mathbb{E}_{s_0}\\left[\\sum^{\\infty}_{k=0}\\gamma^k\\rho^{\\pi_\\theta}(s_{0} \\rightarrow s, k)\\right]\\phi(s)\\\\\n",
    "&= \\sum_{s}\\eta(s)\\phi(s)\\\\\n",
    "&= \\left(\\sum_{s}\\eta(s)\\right)\\sum_{s}\\frac{\\eta(s)}{\\sum_{s}\\eta(s)}\\phi(s)\\\\\n",
    "&\\propto \\sum_{s}\\frac{\\eta(s)}{\\sum_{s}\\eta(s)}\\phi(s)\\\\\n",
    "&= \\sum_{s}\\nu^{\\pi_\\theta}(s)\\sum_{a}Q^{\\pi_\\theta}(s,a)\\nabla_{\\theta}\\pi_{\\theta}(a|s)\n",
    "\\end{align}\n",
    "$$\n",
    "证明完毕！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** 以优势函数$A(s_t,a_t)$为例，请说明为什么沿着梯度$g$的方向优化就可以学到一个收益更大的策略。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
