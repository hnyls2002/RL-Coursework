{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB23C3A83BD146A58F3BB71296D00045",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Model-Free TD Control: Q-Learning and SARSA\n",
    "\n",
    "**本notebook包含2个练习和5个代码填空。**\n",
    "\n",
    "\n",
    "## 在线策略学习与离线策略学习\n",
    "称进行值函数($V^{\\pi}(s)$ 或 $Q^{\\pi}(s,a)$)评估的策略为目标策略 $\\pi(a|s)$，收集数据的策略为行为策略 $\\mu(a|s)$。\n",
    "\n",
    "1. 在线策略学习：目标策略与行为策略一致。其典型例子是 SARSA 算法，基于当前策略直接执行一次动作选择，然后用该采样更新当前的策略。该方法无法兼顾探索(exploration)和利用(exploitation)。光利用当前最优选择，可能收敛到局部最优，而加入探索又降低了学习效率。$\\epsilon$-greedy 算法是对这一矛盾的折衷。在线策略学习直接了当，速度快，但不一定找到最优策略。\n",
    "\n",
    "2. 离线策略学习：目标策略与行为策略不一致。其典型例子是 Q-learning 算法，更新目标策略时，直接选择最优动作，而行为策略并不如此，从而可以产生某概率分布下的大量行为数据，利于探索。当然这么做是需要满足数学条件的，可以从 $\\mu$ 学到 $\\pi$ 的条件是 $\\pi(a|s)>0\\Rightarrow\\mu(a|s)>0$ 。\n",
    "\n",
    "3. 两种学习策略的关系：在线策略学习是离线策略学习的特殊情形，其目标策略和行为策略相同。离线策略学习的劣势是收敛慢，优势则是更为强大和通用，因为它确保了数据全面性，所有行为都能覆盖。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DFD21A3032847B882971A6B91E5F741",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 运行环境：Cliff Walking\n",
    "\n",
    "![Cliff%20Walking.png](images/Cliff%20Walking.png)\n",
    "\n",
    "该环境在每个位置通常有上下左右四种操作，在悬崖格子上的奖励是-100，其他格子的奖励是-1。而且如果走到了悬崖格子，会被直接传送到起点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "9929D333324342078BF6342FAB11C9B9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, length, height):\n",
    "        self.length = length        #模型的长，我们采用16\n",
    "        self.height = height        #模型的宽，我们采用6\n",
    "        self.x = 0                  #记录当前位置横坐标\n",
    "        self.y = 0                  #记录当前位置纵坐标\n",
    "\n",
    "    def step(self, action): #外部调用这个函数来让当前位置改变\n",
    "        \"\"\"4 legal actions, 0:up, 1:down, 2:left, 3:right\"\"\"\n",
    "        change = [[0, 1], [0, -1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.height - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.length - 1, max(0, self.y + change[action][1]))\n",
    "\n",
    "        states = [self.x, self.y]\n",
    "        reward = -1\n",
    "        terminal = False\n",
    "        if self.x == 0: \n",
    "            if self.y > 0:\n",
    "                terminal = True\n",
    "                if self.y != self.length - 1:\n",
    "                    reward = -100\n",
    "        return reward, states, terminal\n",
    "\n",
    "    def reset(self): #交互程序回归初始状态\n",
    "        self.x = 0\n",
    "        self.y = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于该环境，首先定义基本算法框架。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "7DE5FB75264746CE8EB8ED8DBB7DD0E8",
    "jupyter": {},
    "notebookId": "604f2dd7df26380015a09910",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Base_Q_table():\n",
    "    def __init__(self, length, height, actions=4, alpha=0.1, gamma=0.8):\n",
    "        self.table = np.zeros((length * height, actions))\n",
    "        self.actions = actions\n",
    "        self.length = length\n",
    "        self.height = height\n",
    "        self.alpha = alpha #学习率\n",
    "        self.gamma = gamma #衰减参数\n",
    "\n",
    "    def query_q(self, action, state):\n",
    "        return self.table[state[0] * self.length + state[1], action]\n",
    "    \n",
    "    def update_q(self, action, state, q):\n",
    "        self.table[state[0] * self.length + state[1], action] = q\n",
    "\n",
    "    def best_action(self, state):\n",
    "        return np.argmax(self.table[state[0] * self.length + state[1]])\n",
    "\n",
    "    def epsilon(self, num_episode): #我们这里采用了衰减的epsilon以获得相对优秀的收敛效果\n",
    "        return min(0.5, 20. / (num_episode + 1))\n",
    "\n",
    "    def max_q(self, state):\n",
    "        action = self.best_action(state)\n",
    "        return self.query_q(action, state)\n",
    "        \n",
    "    def take_action(self, state, num_episode): #选取下一步的操作\n",
    "        ########################################\n",
    "        ## Programming 1: $\\epsilon$ - greedy 选取动作\n",
    "        ########################################\n",
    "        action = 0\n",
    "        ########################################\n",
    "        ## End of Programming 1\n",
    "        ########################################\n",
    "        return action\n",
    "\n",
    "    def update(self, action, next_action, state, next_state, reward, is_done):\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13C830FB004544F68B80FAFED7DB1BBE",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## SARSA算法\n",
    "\n",
    "算法介绍\n",
    "1. 对于当前策略执行获得的每个 SARSA 五元组，其中 s 指状态，a 指动作，r 指回报，每次转移涉及的五个量构成了它的名称\n",
    "2. SARSA 是对状态-动作值函数进行更新\n",
    "3. 是一种 On-policy Control 的方法\n",
    "4. 是一种模型无关的方法\n",
    "\n",
    "使用SARSA的在线策略控制 on-policy\n",
    "1. 策略评估：$Q(s,a)\\leftarrow Q(s,a)+\\alpha[r+\\gamma Q(s^{\\prime},a^{\\prime})-Q(s,a)]$\n",
    "2. 策略改进：$\\epsilon$-greedy 策略改进\n",
    "3. 作为在线策略学习，SARSA 中两个\"a\"都来自当前策略\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40D864F5AB1346E4929F0D2226E748CB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## SARSA实战代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "7448457EEE784F869E2FD9409870739B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Q_table_sarsa(Base_Q_table):\n",
    "    def __init__(self, length, height, actions=4, alpha=0.1, gamma=0.9):\n",
    "        super().__init__(length, height, actions, alpha, gamma)\n",
    "\n",
    "    def update(self, action, next_action, state, next_state, reward, is_done):\n",
    "        ########################################\n",
    "        ## Programming 2: 更新Q函数表self.table\n",
    "        ########################################\n",
    "        pass\n",
    "        ########################################\n",
    "        ## End of Programming 2\n",
    "        ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们在CliffWalking环境中测试一下SARSA算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "B60D9FA6388C4EFC83F73FFEB6C18296",
    "jupyter": {},
    "notebookId": "6062c0c47652740017163078",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cliff_walk(algo: Base_Q_table, length: int, height: int, label:str, show=True, **kwargs):\n",
    "    val = [0] * 150\n",
    "    x = [0] * 150\n",
    "    num_episode = 3000\n",
    "    env = Env(length=length, height=height)\n",
    "    agent = algo(length=length, height=height, **kwargs)\n",
    "    for episode in range(num_episode):\n",
    "        episodic_reward = 0\n",
    "        is_done = False\n",
    "        state = [0, 0]\n",
    "        action = agent.take_action(state, episode)\n",
    "        while not is_done:\n",
    "            reward, next_state, is_done = env.step(action)\n",
    "            next_action = agent.take_action(next_state, episode)\n",
    "            episodic_reward += reward\n",
    "            agent.update(action, next_action, state, next_state, reward, is_done)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        if episode % 20 == 0:\n",
    "            val[(int)(episode/20)] = episodic_reward\n",
    "        env.reset()\n",
    "    for i in range((int)(num_episode/20)):\n",
    "        x[i] = i\n",
    "    plt.plot(x, val, ls=\"-\", lw=2, label=label)\n",
    "    if show:\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliff_walk(Q_table_sarsa, 16, 6, \"sarsa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DE8B248EFBA41B6A8E1211D397154B6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 多步SARSA\n",
    "\n",
    "类似SARSA算法，多步（n-step）SARSA算法就是每次并不是根据上一步的数据更新，而是根据之前 n 步的数据更新。下面以 n=5 为例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D14CB181112F4688AB87B48E748B7D84",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## n-step SARSA实战代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "4DB83C64EAFA42C495A35C2DF983A8C1",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Q_table_nstep_sarsa(Base_Q_table):\n",
    "    def __init__(self, length, height, actions=4, alpha=0.02, gamma=0.9, n=5):\n",
    "        # n: n-step 长度\n",
    "        super().__init__(length, height, actions, alpha, gamma)\n",
    "        self.n = n\n",
    "        self.state_list = []  # 保存之前的状态\n",
    "        self.action_list = []  # 保存之前的动作\n",
    "        self.reward_list = []  # 保存之前的奖励\n",
    "\n",
    "    def getval(self, t, n):   \n",
    "        ########################################\n",
    "        ## Programming 3: 计算 n-step Return $G_{t:t+n-1}$\n",
    "        ########################################\n",
    "        G = 0\n",
    "        ########################################\n",
    "        ## End of Programming 3\n",
    "        ########################################\n",
    "        return G\n",
    "        \n",
    "    def update(self, action, next_action, state, next_state, reward, is_done): \n",
    "        # Hint: 每次保留n次的数据，从n次之前的数据来更新，可以看到收敛慢了一些，但是相对稳定。\n",
    "        # 注意达到终止状态时的更新情况\n",
    "        ########################################\n",
    "        ## Programming 4: 更新Q函数表self.table\n",
    "        ########################################\n",
    "        pass\n",
    "        ########################################\n",
    "        ## End of Programming 4\n",
    "        ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliff_walk(Q_table_nstep_sarsa, 16, 6, 'n-step-sarsa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37A050467BFC43058FAC45F732E668F4",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "604f2dd7df26380015a09910",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Ex1 证明：**\n",
    "在价值函数V不更新的假设下，\n",
    "$$\n",
    "G_{t: t+n}-V_{t+n-1}\\left(S_{t}\\right) =\\sum_{k=t}^{t+n-1} \\gamma^{k-t} \\delta_{k}~，\n",
    "$$\n",
    "其中$\\delta_{t} = R_{t+1}+\\gamma V_{t}\\left(S_{t+1}\\right)-V_{t}\\left(S_{t}\\right)$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "336D4C26ED9440A58A919481CE295EC6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Q-Learning算法\n",
    "\n",
    "Q-learning算法是一种离线时序差分控制算法, 是无模型强化学习中一个较早的模型, 模型如下:\n",
    "\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a\\in \\mathcal{A}}Q(s_{t+1},a) - Q(s_t,a_t) \\right].\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71A674F3B26C4A4E858648AAD808D19B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Q-Learning实战代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "BD664A78753943BAB485F9AE35B9E80F",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Q_table(Base_Q_table):\n",
    "    def __init__(self, length, height, gamma, actions=4, alpha=0.005):\n",
    "        super().__init__(length, height, actions, alpha, gamma)\n",
    "\n",
    "    def update(self, action, next_action, state, next_state, reward, is_done):\n",
    "        ########################################\n",
    "        ## Programming 5: 更新table\n",
    "        ########################################\n",
    "        pass\n",
    "        ########################################\n",
    "        ## End of Programming 5\n",
    "        ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliff_walk(Q_table, length=16, height=6, gamma=0.8, label=\"Q learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D101F767A7574A2EAB318C48D0109BF5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 下面是不同gamma对收敛率的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [0.1, 0.4, 0.7, 0.95, 1]:\n",
    "    cliff_walk(Q_table, length=16, height=6, gamma=gamma, label=f\"gamma-{gamma}\", show=False)\n",
    "    print(f\"walk finished: gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B74B31B9A3B42579B2DB7C86AC37B29",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "604f2dd7df26380015a09910",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Ex2 请简要说明Q-learning算法和SARSA算法的异同之处，并列举说明两种算法在不同情况下的优劣之处。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08EE8D0E6E33474AA5A9CCF1C18E4FDE",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 总结\n",
    "SARSA 和 Q-Learning 算法是强化学习中两个非常基础的算法，也是在实践过程中比较好用的算法。不同的学习率 $\\alpha$ 和超参数 $\\gamma$ 对收敛速率的影响比较大，但如果学习率过高就可能无法达到最优解。$\\epsilon$-greedy算法中的 $\\epsilon$ 也是非常重要的参量，对平衡探索和利用的关系非常重要。值得注意的是，尽管离线策略学习可以让智能体基于经验回放池中的样本来学习，但需要保证智能体在学习的过程中可以不断和环境进行交互，将采样得到的最新的经验样本加入经验回放池中，从而使经验回放池中有一定数量的样本和当前智能体策略对应的数据分布保持很近的距离。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
