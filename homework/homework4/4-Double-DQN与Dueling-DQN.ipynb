{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_type": "code",
    "id": "71B2C03334E24D66B5E6DB93C8387937",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Double DQN && Dueling DQN\n",
    "\n",
    "本次作业包含3个代码填空和1个Exercise。\n",
    "\n",
    "# 简介\n",
    "DQN算法敲开了用深度神经网络构建Q函数的大门，但是作为先驱性的工作，其本身也存在着一些问题以及一些可以改进的地方，于是在DQN之后学术界涌现出了非常多的改进算法。在本节课程中，我们将介绍其中两个非常著名的算法：Double DQN和Dueling DQN，这两个算法实现简单但能一定程度改善DQN的效果。如果想要了解更加详细的各种DQN改进方法，我们推荐读者阅读论文[1]以及其中的引用文献。\n",
    "\n",
    "[1] Hessel, Matteo, et al. \"[Rainbow: Combining improvements in deep reinforcement learning.](https://arxiv.org/abs/1710.02298)\" arXiv preprint arXiv:1710.02298 (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D47F7231B53449B98161F7E4E8B91F15",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Double DQN\n",
    "\n",
    "传统的DQN算法通常会导致对Q值的过高估计。我们可以观察到，传统DQN优化的目标为\n",
    "$$\n",
    "r+\\gamma \\max _{a^{\\prime}} Q_{\\theta^-}\\left(s', a^{\\prime}\\right)\n",
    "$$\n",
    "这个目标由目标网络（参数为$\\theta^-$）计算得出，我们还可以将其写成如下形式\n",
    "\n",
    "$$\n",
    "Q_{\\theta^-}\\left(s', \\underset{a'}{\\operatorname{argmax}} Q_{\\theta^-}\\left(s',  a' \\right) \\right)\n",
    "$$\n",
    "\n",
    "换句话说，max操作实际可以被拆解为两部分，首先选取状态$s'$下的最优动作$a^{*} = \\underset{a'}{\\operatorname{argmax}} Q_{\\theta^-}\\left(s', a' \\right)$，接着计算该动作对应的Q值$Q\\left(s', a^* \\right)$。\n",
    "当这两部分采用同一套Q网络进行计算时，每次得到的都是神经网络当前估算的所有动作Q值中的最大值。考虑到我们通过神经网络估算的Q值本身在某些时候会产生或正或负的误差，在DQN的更新方式下神经网络会将正误差累积。比如我们考虑一个特殊情形，在状态$s'$下所有动作的Q值均为0，即$Q(s', a_i)=0,\\forall i$，此时正确的更新目标应为$r+0=r$。但是由于神经网络拟合的误差通常会出现某些动作的估算有正误差$Q(s', a_j) > 0$，此时我们的更新目标出现了过高估计，$r+\\gamma \\max Q \\geq r$。当我们用DQN的更新公式进行更新时，$Q(s, a)$也就会被过高估计了。同理，我们拿这个$Q(s,a)$来作为更新目标来更新上一步的Q值时，同样会过高估计，并且误差将会逐步累积。\n",
    "\n",
    "\n",
    "为了解决这一问题，[Double DQN](https://arxiv.org/abs/1509.06461)算法提出利用两个独立训练的神经网络估算$\\max_{a'} Q_*(s', a')$。具体做法是将原有的$\\max _{a^{\\prime}} Q_{\\theta^-}\\left(s', a^{\\prime}\\right)$更改为\n",
    "\n",
    "$$\n",
    "Q_{\\theta^-}\\left(s', \\underset{a'}{\\operatorname{argmax}} Q_{\\theta}\\left(s',  a' \\right) \\right)\n",
    "$$\n",
    "\n",
    "即我们利用一套神经网络$Q_{\\boldsymbol{\\theta}}$选取动作，用另一套神经网络$Q_{\\boldsymbol{\\theta}^{-}}$计算该动作的Q值。这样，就算其中一套神经网络某个动作存在比较严重的过高估计，由于另一套神经网络的存在，这个动作最终使用的Q值不会存在很大的过高估计，从一定程度上解决了这个问题。\n",
    "\n",
    "在传统的DQN算法中，本来就存在两套Q函数的神经网络——目标网络和训练网络，只不过$\\max_{a'} Q_{\\theta^-}\\left(s', a' \\right)$的计算只用到了其中的目标网络，那么我们恰好可以直接将训练网络作为Double DQN算法中的第一套神经网络选取动作，将目标网络作为第二套神经网络计算Q值，这便是Double DQN的主要思想。\n",
    "由于在DQN算法中我们就将训练网络的参数记为$\\theta$，将目标网络的参数记为$\\theta^{-}$，这与之前Double DQN中写下的两套神经网络的参数是统一的，我们可以直接写出如下Double DQN的优化目标\n",
    "\n",
    "$$\n",
    "r+\\gamma Q_{\\theta^-}\\left(s', \\underset{a'}{\\operatorname{argmax}} Q_\\theta\\left(s', a'\\right)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/qipg4vdji5.png?imageView2/0/w/960/h/960)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB4366B054774487A4C29B270E018DCA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Double DQN代码实践\n",
    "显然，Double DQN与DQN的差别只是在于计算状态$s'$下Q值时动作的选取上：\n",
    "* DQN的优化目标可以写为$r+\\gamma Q_{\\theta^-}\\left(s', \\underset{a'}{\\operatorname{argmax}} Q_{\\theta^-}\\left(s', a'\\right)\\right)$，动作的选取依靠目标网络$Q_{\\theta^-}$\n",
    "* Double DQN的优化目标为$r+\\gamma Q_{\\theta^-}\\left(s, \\underset{a'}{\\operatorname{argmax}} Q_\\theta\\left(s', a' \\right)\\right)$，动作的选取依靠训练网络$Q_\\theta$\n",
    "\n",
    "所以代码实现可以直接在原DQN的基础上进行，无需做过多修改。\n",
    "\n",
    "本次采用的环境是[Pendulum](https://gym.openai.com/envs/Pendulum-v0/)，该环境下有一个从随机位置开始的钟摆，钟摆向上保持直立时奖励为0，钟摆在其他位置时奖励为负数，环境本身没有终止状态。环境的状态为钟摆角度的正弦值$\\sin \\varphi$，余弦值$\\cos \\varphi$，角速度$\\dot\\varphi$，动作为对钟摆施加的力矩。每一步会根据当前摆的状态的好坏给予不同的奖励,该环境的奖励函数为$-(\\varphi^2+0.1\\dot\\varphi^2+0.001a^2)$。200步后自动结束游戏。在此环境下我们可以验证DQN对于Q值的过高估计——该环境下Q值的最大估计应为0(钟摆向上保持竖直时能选取的最大Q值)，Q值出现大于0的情况则说明出现了过高估计。\n",
    "**注：由于Pendulum-v1环境的动作为一个代表力矩，在[-2,2]范围内的连续值，为了方便起见，我们采用离散化动作的方法。如下面的代码中我们将连续的动作空间离散为11个动作。动作0，1，..., 9, 10分别代表力矩为-2， -1.6， ..., 1.6, 2**\n",
    "\n",
    "该环境的状态有三个参数：\n",
    "\n",
    "标号 | 名称  | 最小值 | 最大值  \n",
    "----|--------------|-----|----   \n",
    "0   | $\\cos \\varphi$  | -1.0| 1.0\n",
    "1   | $\\sin \\varphi$   | -1.0| 1.0\n",
    "2   | $\\dot\\varphi$  | -8.0| 8.0\n",
    "\n",
    "该环境只有一类动作（连续动作）：\n",
    "\n",
    "标号 | 动作  | 最小值 | 最大值  \n",
    "----|--------------|-----|----   \n",
    "0   | 力矩 | -2.0| 2.0\n",
    "\n",
    "\n",
    "\n",
    "![Pendulum](https://cdn.kesci.com/upload/image/qfoepa3n0z.gif?imageView2/0/w/960/h/960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install gym\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!{sys.executable} -m pip install -U matplotlib\n",
    "    \n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "0C060B4A6EB045CA8BD040AC7ED584FE",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "# 注意此时的训练函数多了一个isDoubleDQN的参数\n",
    "def learn(batch_size, current_model, target_model, replay_buffer, optimizer, isDoubleDQN):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        state = torch.FloatTensor(np.float32(state)).to(device)\n",
    "        next_state = torch.FloatTensor(np.float32(next_state)).to(device)\n",
    "        action = torch.LongTensor(action).to(device)\n",
    "        reward = torch.FloatTensor(reward).to(device)\n",
    "        done = torch.FloatTensor(done).to(device)\n",
    "\n",
    "    q_values = current_model(state)\n",
    "    next_q_values = current_model(next_state)\n",
    "    next_q_values_target = target_model(next_state)\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # the only difference between Double DQN and DQN\n",
    "    ########################################\n",
    "    # Programming 1: 计算DQN与DoubleDQN算法的更新目标，并更新网络\n",
    "    ########################################\n",
    "    \n",
    "    ########################################\n",
    "    ## End of Programming 1\n",
    "    ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "34ADEBEFA82E4EE6816D2235C7F53B80",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2353A27C08C2420589794B5F7E60E8A7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN_base(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state, epsilon, discrete_action_n):\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(np.float32(state)).unsqueeze(0).to(device)\n",
    "                q_value = self.forward(state)\n",
    "                action = q_value.max(1).indices.item()\n",
    "        else:\n",
    "            action = random.randrange(discrete_action_n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "FA505EC6C40B49AC826647CCEBBCEB73",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN(DQN_base):\n",
    "    def __init__(self, input_n, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.input_n = input_n\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_n, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, self.num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "FCBE3D47D48E4146A008DC91678CD05A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discrete_action_n = 11  # 使用离散的动作空间 [0, discrete_action_n-1]\n",
    "env_id = \"Pendulum-v1\"\n",
    "env = gym.make(env_id)\n",
    "input_n = env.observation_space.shape[0]\n",
    "action_lowbound = env.action_space.low[0]\n",
    "action_upbound = env.action_space.high[0]\n",
    "# discrete action to continuous action\n",
    "def faction(discrete_n):\n",
    "    return action_lowbound + (discrete_n / (discrete_action_n - 1)) * (action_upbound - action_lowbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "DDF5D9CCE3E545F9B52BE405EF6014B9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6152DD2FF10647149CEDE263C795118D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotMaxQ(max_Qs):\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.ylabel(\"Q value\")\n",
    "    plt.plot(max_Qs)\n",
    "    plt.show()\n",
    "    \n",
    "def plotReward(all_rewards):\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.plot(all_rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "D7E1866C909944B6855A060A5BA21FDC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smooth(array, l=10):\n",
    "    return [np.mean(array[i-l+1:i+1]) for i in range(len(array))]\n",
    "\n",
    "def train(isDoubleDQN: bool=False, isDuelingDQN: bool=False, DQN: DQN_base=None, DuelingDQN: DQN_base=None):\n",
    "    # 超参数定义\n",
    "    replay_initial = 3000\n",
    "    replay_buffer_size = 3000\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "        \n",
    "    num_frames = 50000\n",
    "    batch_size = 32\n",
    "        \n",
    "    epsilon_start = 1.0\n",
    "    epsilon_final = 0.01\n",
    "    epsilon_decay = 1500\n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(\n",
    "            -1. * frame_idx / epsilon_decay)\n",
    "    update_target_step = 200\n",
    "\n",
    "    def maxQ(model, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(np.float32(state)).unsqueeze(0).to(device)\n",
    "            q_value = model.forward(state)\n",
    "            max_q_value = q_value.max(1).values.item()\n",
    "        return max_q_value\n",
    "    \n",
    "    if isDuelingDQN:\n",
    "        current_model = DuelingDQN(input_n, discrete_action_n).to(device)\n",
    "        target_model = DuelingDQN(input_n, discrete_action_n).to(device)\n",
    "    else:\n",
    "        current_model = DQN(input_n, discrete_action_n).to(device)\n",
    "        target_model = DQN(input_n, discrete_action_n).to(device)\n",
    "    update_target(current_model, target_model)\n",
    "    \n",
    "    optimizer = optim.Adam(current_model.parameters(), lr=0.005)\n",
    "    \n",
    "    max_Qs = []\n",
    "    now_Q = 0\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "    episode = 0\n",
    "\n",
    "    state = env.reset()[0]\n",
    "    random.seed(1)\n",
    "    for frame_idx in tqdm(range(1, num_frames + 1)):\n",
    "        time.process_time()\n",
    "        epsilon = epsilon_by_frame(frame_idx)\n",
    "        action = current_model.act(state, epsilon, discrete_action_n)\n",
    "    \n",
    "        now_Q = maxQ(current_model, state) * 0.01 + now_Q * 0.99 # to make the curve more smooth\n",
    "        max_Qs.append(now_Q)\n",
    "    \n",
    "        next_state, reward, terminated, truncated, _ = env.step(np.array([faction(action)]))\n",
    "        done = terminated or truncated\n",
    "        reward /= 10\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()[0]\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            episode += 1\n",
    "    \n",
    "        if len(replay_buffer) >= replay_initial:\n",
    "            learn(batch_size, current_model, target_model, replay_buffer, optimizer, isDoubleDQN=isDoubleDQN)\n",
    "    \n",
    "        if frame_idx % update_target_step == 0:\n",
    "            update_target(current_model, target_model)\n",
    "        \n",
    "    return max_Qs, all_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下训练预计时间：1分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_Qs_doubleDQN, returns_doubleDQN = train(isDoubleDQN = True, DQN=DQN)\n",
    "# plotMaxQ(max_Qs_doubleDQN)\n",
    "# plotReward(returns_doubleDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_Qs_regularDQN, returns_regularDQN = train(isDoubleDQN = False, DQN=DQN)\n",
    "# plotMaxQ(max_Qs_regularDQN)\n",
    "# plotReward(returns_regularDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1863597A42EC4FDAB425412102F2863E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compareMaxQ(Q1, Q2, labels=[\"Double DQN\", \"DQN\"]):\n",
    "    Q1, Q2 = smooth(Q1,l=50), smooth(Q2, l=50)\n",
    "    plt.xlabel(\"frame\")\n",
    "    plt.ylabel(\"Q value\")\n",
    "    plt.plot(Q1)\n",
    "    plt.plot(Q2)\n",
    "    plt.legend(labels)\n",
    "    plt.show()\n",
    "    \n",
    "def compareReturn(reward1, reward2, labels=[\"Double DQN\", \"DQN\"]):\n",
    "    reward1, reward2 = smooth(reward1), smooth(reward2)\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    minEpisode = min(len(reward1),len(reward2))\n",
    "    plt.plot(reward1[:minEpisode])\n",
    "    plt.plot(reward2[:minEpisode])\n",
    "    plt.legend(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareReturn(returns_doubleDQN, returns_regularDQN)\n",
    "compareMaxQ(max_Qs_doubleDQN, max_Qs_regularDQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5495D724E19046398C7748AB255CFB0A",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "60813b1cb141500017a637fe",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "参考结果如下:\n",
    "\n",
    "![Image Name](images/DoubleDQN_DQN_1.png)\n",
    "\n",
    "![Image Name](images/DoubleDQN_DQN_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C88F499B85ED42C4832BB76FA134C369",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们可以发现，相比于Double DQN，普通的DQN算法对于Q值的估计要略微高一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B83470FC97C04F3EBA8D74E9A398137A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Dueling DQN\n",
    "\n",
    "[Dueling DQN](https://arxiv.org/abs/1511.06581)是另一种DQN的改进算法。它在传统DQN的基础上只进行了稍稍改动，但却能大幅提升DQN的表现。在强化学习中，我们将状态动作价值函数减去状态函数定义为优势函数，即$A(s,a) = Q(s,a)-V(s)$。在同一个状态下，所有动作的优势值之和为0，因为所有动作的动作价值的平均就是这个状态的状态价值。据此，在Dueling DQN中，Q网络被建模为：\n",
    "\n",
    "$$\n",
    "Q(s, a ; \\theta, \\alpha, \\beta)=V(s ; \\theta, \\beta)+A(s, a ; \\theta, \\alpha)\n",
    "$$\n",
    "\n",
    "其中$V(s ; \\theta, \\beta)$为状态价值函数，而$A(s, a ; \\theta, \\alpha)$则为该状态下采取不同动作的优势函数，表征的是采取不同动作的差异性。$\\theta$是状态价值函数和优势函数共享的网络参数，一般为神经网络用来提取特征的前几层。而$\\alpha$和$\\beta$为两者各自的参数。在这样的建模下我们不再让神经网络直接输出Q值，而是训练神经网络的两部分分别输出价值函数和优势函数，再求和得到Q值。DQN和Dueling DQN的网络结构图区别如下：\n",
    "\n",
    "![DQN与Dueling DQN网络结构的区别](https://cdn.kesci.com/upload/image/qfmiegdc53.png?imageView2/0/w/320/h/320)\n",
    "\n",
    "对于上述Dueling DQN中的公式$Q(s, a ; \\theta, \\alpha, \\beta)=V(s ; \\theta, \\beta)+A(s, a ; \\theta, \\alpha)$，它存在对于$A$值和$V$值建模不唯一性的问题（比如对于同样的$Q$值，如果将$V$值加上任意大小的常数$C$，再将所有$A$值减去$C$，则我们得到的$Q$值依然不变，这就给训练造成了不稳定）。为了解决这一问题，原论文将强制将优势函数在实际行为的输出调整为$0$，即\n",
    "$$\n",
    "Q(s, a ; \\theta, \\alpha, \\beta)=V(s ; \\theta, \\beta)+\\left(A(s, a ; \\theta, \\alpha)-\\max _{a^{\\prime}} A\\left(s, a^{\\prime} ; \\theta, \\alpha\\right)\\right)\n",
    "$$\n",
    "此时可以确保$V$值建模的唯一性。\n",
    "而在实际实现过程中，我们通常将优势函数在所有动作的输出期望设为$0$,即\n",
    "$$\n",
    "Q(s, a ; \\theta, \\alpha, \\beta)=V(s ; \\theta, \\beta)+ \n",
    "\\left(A(s, a ; \\theta, \\alpha)-\\frac{1}{|\\mathcal{A}|} \\sum_{a^{\\prime}} A\\left(s, a^{\\prime} ; \\theta, \\alpha\\right)\\right)\n",
    "$$\n",
    "这样可以提高稳定性。在下面的代码实现中，我们将采取此种方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.**  Dueling DQN从单一的网络架构中分离出价值函数和优势函数，这种分离的直观解释是什么。并请解释为何Dueling DQN在一些环境中的表现会好于DQN。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79E9A4FE236A4AB185798B1820CC7854",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Dueling DQN代码实践\n",
    "Dueling DQN与DQN相比的差异只是在网络结构上，大部分代码依然可以继续沿用，注意进行下方代码前要运行上方定义超参数的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "B3F22ADAA3A54F97ADA795A9F52734F9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 注意此时多了一个参数isDuelingDQN\n",
    "class DuelingDQN(DQN_base):\n",
    "    def __init__(self, input_n, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_n = input_n\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # the difference\n",
    "        ########################################\n",
    "        ## Programming 2: 定义Dueling DQN的网络结构，建议参考上方DQN的网络结构所使用的超参数\n",
    "        ########################################\n",
    "\n",
    "        ########################################\n",
    "        ## End of Programming 2\n",
    "        ########################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the difference\n",
    "        ########################################\n",
    "        ## Programming 3: 实现Dueling DQN输出Q的方法\n",
    "        ########################################\n",
    "        pass\n",
    "        ########################################\n",
    "        ## End of Programming 3\n",
    "        ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下训练预计时间：2分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, returns_duelingDQN = train(isDoubleDQN=False, isDuelingDQN=True, DQN=DQN, DuelingDQN=DuelingDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, returns_DQN = train(isDoubleDQN=False, isDuelingDQN=False, DQN=DQN, DuelingDQN=DuelingDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareReturn(returns_duelingDQN, returns_DQN, labels=['Dueling DQN', 'DQN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1804E104A298468F8BFBFE09F880EB7E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "通过曲线可以看出，相比于朴素的DQN，Dueling DQN相比于DQN在多个动作选择下的学习显得更加稳定一些。但是由于该环境较为简单，动作空间也不大，所以优势并不明显。\n",
    "\n",
    "参考结果：\n",
    "\n",
    "![Image Name](images/dueling_dqn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D53DD29948FB42ED81E45AD5BE159B59",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 总结\n",
    "在传统的DQN基础上，有两种非常容易实现的变式——Double DQN和Dueling DQN，Double DQN解决了DQN中对Q值的过高估计，而Dueling DQN能够很好地学习到不同动作的差异性，在动作空间较大的环境下非常有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81902069238D479588134D7F7A7371AC",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "5f8f31e546ba5e0030780165",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 扩展阅读： 对Q值过高估计的定量分析\n",
    "我们可以对Q值的过估计做一个简化的定量分析。假设在状态$s$下所有动作的期望回报均无差异，即$Q_*(s,a) = V_*(s)$（此情形是为了定量分析所简化的情形，实际上不同动作的期望回报通常会存在差异）。假设神经网络估算误差$Q_{\\theta^-}(s, a)-V_{*}$服从$[-1,1]$之间的均匀独立同分布。假设动作空间大小为$m$，那么对于任意状态$s$我们有\n",
    "$$\n",
    "\\mathbb{E}\\left[\\max _{a} Q_{\\theta^-}(s, a)- \\max_{a'} Q_{*}(s,a')\\right]=\\frac{m-1}{m+1}\n",
    "$$\n",
    "**证明：**\n",
    "我们将估算误差记为$\\epsilon_{a}=Q_{\\theta^-}(s, a)-\\max_{a'}Q_{*}(s, a')$，由于估算误差对于不同的动作是独立的，我们有\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P\\left(\\max _{a} \\epsilon_{a} \\leq x\\right) =\\prod_{a=1}^{m} P\\left(\\epsilon_{a} \\leq x\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "$P(\\epsilon_a \\leq x)$是$\\epsilon_a$的累积分布函数（CDF）,它可以具体被写为\n",
    "$$\n",
    "P\\left(\\epsilon_{a} \\leq x\\right)=\\left\\{\\begin{array}{ll}\n",
    "0 & \\text { if } x \\leq-1 \\\\\n",
    "\\frac{1+x}{2} & \\text { if } x \\in(-1,1) \\\\\n",
    "1 & \\text { if } x \\geq 1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "因此，我们有关于$\\max_a \\epsilon_a$的累积密度函数(CDF)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P\\left(\\max _{a} \\epsilon_{a} \\leq x\\right) &=\\prod_{a=1}^{m} P\\left(\\epsilon_{a} \\leq x\\right) \\\\\n",
    "&=\\left\\{\\begin{array}{ll}\n",
    "0 & \\text { if } x \\leq-1 \\\\\n",
    "\\left(\\frac{1+x}{2}\\right)^{m} & \\text { if } x \\in(-1,1) \\\\\n",
    "1 & \\text { if } x \\geq 1\n",
    "\\end{array}\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "最后我们可以得到\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[\\max _{a} \\epsilon_{a}\\right] &=\\int_{-1}^{1} x \\frac{\\mathrm{d}}{\\mathrm{d} x} P\\left(\\max _{a} \\epsilon_{a} \\leq x \\right) \\mathrm{d} x \\\\\n",
    "&=\\left[\\left(\\frac{x+1}{2}\\right)^{m} \\frac{m x-1}{m+1}\\right]_{-1}^{1} \\\\\n",
    "&=\\frac{m-1}{m+1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "虽然这一分析简化了实际环境，但它正确刻画了Q值过高估计的一些性质，比如Q值的过高估计随动作数目$m$的增加而增加，换而言之，在动作选择数更多的环境中Q值的过高估计会更严重。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
